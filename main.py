# -*- coding: utf-8 -*-
"""
AlphaFactorLab æ•°æ®æ›´æ–°ä¸»ç¨‹åº (ç²¾ç¡®è¦†ç›–ç‰ˆ)
åŠŸèƒ½: è°ƒåº¦å„ä¸ª Fetcherï¼Œæ¸…æ´—æ•°æ®ï¼Œå¹¶å­˜å‚¨ä¸º Hive Partition Parquet
ç‰¹ç‚¹: ä½¿ç”¨ {key_col}.parquet å‘½åæ–‡ä»¶ï¼Œæ”¯æŒå¹‚ç­‰å†™å…¥ï¼Œæ— éœ€æ¸…ç©ºæ–‡ä»¶å¤¹å³å¯å»é‡
ç”¨æ³•:
    python main.py --task all --mode update
    python main.py --task stock --mode full
"""

import argparse
import datetime
import time
from tqdm import tqdm
from typing import Tuple

# --- å¯¼å…¥é…ç½® ---
from config.settings import ETF_POOL, INDEX_POOL, START_DATE_FULL, PROCESSED_DIR

# å¼•å…¥æ ¸å¿ƒæ¨¡å—
from src.fetchers.baostock_api import BaostockFetcher
from src.fetchers.akshare_api import AkshareFetcher
from src.fetchers.mootdx_api import MootdxFetcher
from src.processors.cleaner import DataCleaner
from src.storage.parquet_manager import ParquetStorage
from src.utils.logger import get_logger

# é…ç½®æ—¥å¿—
logger = get_logger("Main", "data_update.log")

def get_date_range(mode: str) -> Tuple[str, str]:
    """
    è®¡ç®—æ—¶é—´èŒƒå›´
    full: 1990-12-19 -> ä»Šå¤©
    update: ä»Šå¹´1æœˆ1æ—¥ -> ä»Šå¤© 
            (é…åˆæ–‡ä»¶åè¦†ç›–ç­–ç•¥ï¼Œæ¯æ¬¡æ›´æ–°é‡è·‘å½“å¹´æ•°æ®ï¼Œç¡®ä¿æ•°æ®ä¿®æ­£ä¸”æ— é‡å¤)
    """
    end_date = datetime.date.today().strftime('%Y-%m-%d')
    
    if mode == 'full':
        start_date = START_DATE_FULL
    else:
        # update æ¨¡å¼: è·å–å½“å¹´çš„1æœˆ1æ—¥
        current_year = datetime.date.today().year
        start_date = f"{current_year}-01-01"
        
    return start_date, end_date

# ==========================================
# 1. ğŸ“ˆ è‚¡ç¥¨ä¸æŒ‡æ•° (Baostock)
# ==========================================
def run_stock_update(mode: str):
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting STOCK update ({mode}): {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    
    with BaostockFetcher() as bs:
        # 1.1 æ›´æ–°æŒ‡æ•° (Index)
        logger.info("Updating Indexes...")
        for code in INDEX_POOL:
            # è·å–æŒ‡æ•°æ—¥çº¿ (ä¸å¤æƒ)
            df = bs.fetch_index_kline(code, start_date, end_date)
            if not df.empty:
                df = cleaner.clean_daily_market_data(df)
                # æŒ‡æ•°æŒ‰ä»£ç å‘½å: sh.000001.parquet
                storage.save_partitioned(df, "index_price_daily", key_col='code')
        
        # 1.2 æ›´æ–°ä¸ªè‚¡ (Stock)
        raw_codes = bs.fetch_all_stock_codes()
        
        # --- è¿‡æ»¤æŒ‡æ•°ä»£ç  ---
        # å‰”é™¤ 'sh.000' å’Œ 'sz.399' å¼€å¤´çš„æŒ‡æ•°
        stock_codes = [
            c for c in raw_codes 
            if not (c.startswith("sh.000") or c.startswith("sz.399"))
        ]
        logger.info(f"Found {len(raw_codes)} codes, filtered to {len(stock_codes)} stocks.")
        
        for code in tqdm(stock_codes, desc="Stocks"):
            try:
                # adjust='1' è¡¨ç¤ºåå¤æƒ
                df = bs.fetch_daily_kline(code, start_date, end_date, adjust='1')
                if not df.empty:
                    df = cleaner.clean_daily_market_data(df)
                    # ä¸ªè‚¡æŒ‰ä»£ç å‘½å: sh.600000.parquet
                    # å³ä½¿å¤šæ¬¡è¿è¡Œï¼ŒåŒåæ–‡ä»¶ä¼šè¢«è¦†ç›–ï¼Œå®ç°å¤©ç„¶å»é‡
                    storage.save_partitioned(df, "stock_price_daily", key_col='code')
            except Exception as e:
                logger.error(f"Failed stock {code}: {e}")

# ==========================================
# 2. ğŸ“Š ETF (Mootdx)
# ==========================================
def run_etf_update(mode: str):
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting ETF update ({mode}): {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    
    with MootdxFetcher() as mdx:
        for name, (code, ipo_year) in tqdm(ETF_POOL.items(), desc="ETFs"):
            try:
                # adjust_factor='02' è¡¨ç¤ºåå¤æƒ
                df = mdx.fetch_etf_daily_kline(code, ipo_year, start_date, end_date, adjust_factor='02')
                if not df.empty:
                    df['name'] = name 
                    df = cleaner.clean_daily_market_data(df)
                    # ETF æŒ‰åç§°å‘½å: HS300.parquet (ç›´è§‚æ˜“è¯»)
                    storage.save_partitioned(df, "etf_price_daily", key_col='name')
            except Exception as e:
                logger.error(f"Failed ETF {name}: {e}")

# ==========================================
# 3. ğŸ’° è´¢åŠ¡ä¸æ¦‚å¿µ (Akshare)
# ==========================================
def run_finance_update(mode: str):
    logger.info(f"ğŸš€ Starting FINANCE & CONCEPT update")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    # 3.1 è´¢åŠ¡æ•°æ®
    with BaostockFetcher() as bs:
        raw_codes = bs.fetch_all_stock_codes()
        stock_codes = [c for c in raw_codes if not (c.startswith("sh.000") or c.startswith("sz.399"))]
        
    logger.info(f"Updating Financial Reports for {len(stock_codes)} stocks...")
    for code in tqdm(stock_codes, desc="Finance"):
        try:
            # è´¢æŠ¥é€šå¸¸è¿”å›å†å²æ‰€æœ‰æ•°æ®ï¼Œæ‰€ä»¥ mode å‚æ•°å½±å“ä¸å¤§ï¼Œæ€»æ˜¯å…¨é‡è¦†ç›–å•è‚¡æ–‡ä»¶
            df = ak_fetcher.fetch_financial_report(code)
            if not df.empty:
                df = cleaner.clean_financial_report(df)
                # æŒ‰æŠ¥å‘ŠæœŸå¹´ä»½åˆ†åŒºï¼Œæ–‡ä»¶åä¸º code.parquet (e.g. year=2023/sh.600000.parquet)
                storage.save_partitioned(df, "stock_financial", partition_col="report_date", key_col='code')
        except Exception:
            pass

    # 3.2 æ¦‚å¿µæ¿å—
    logger.info("Updating Concepts...")
    df_concepts = ak_fetcher.fetch_concept_boards()
    if not df_concepts.empty:
        start_date, end_date = get_date_range(mode)
        start_str = start_date.replace("-", "")
        end_str = end_date.replace("-", "")
        
        for _, row in tqdm(df_concepts.iterrows(), total=len(df_concepts), desc="Concept Daily"):
            name = row['name']
            try:
                df_daily = ak_fetcher.fetch_concept_daily(name, start_str, end_str)
                if not df_daily.empty:
                    df_daily['concept_name'] = name
                    df_daily = cleaner.clean_daily_market_data(df_daily)
                    # æ¦‚å¿µæŒ‰åç§°å‘½å: é”‚ç”µæ± .parquet
                    storage.save_partitioned(df_daily, "concept_price_daily", key_col='concept_name')
                time.sleep(0.5) 
            except Exception:
                pass

# ==========================================
# 4. ğŸ—ï¸ å¦ç±»æ•°æ®
# ==========================================
def run_alt_update(mode: str):
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting ALTERNATIVE update: {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    start = datetime.datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.datetime.strptime(end_date, "%Y-%m-%d")
    date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days + 1)]
    
    for date_obj in tqdm(date_generated, desc="Daily Alt Data"):
        date_str = date_obj.strftime("%Y%m%d")
        
        # 4.1 æ–°é—»è”æ’­ (æŒ‰æ—¥å­˜å‚¨)
        try:
            df_news = ak_fetcher.fetch_cctv_news(date_str)
            if not df_news.empty:
                df_news = cleaner.clean_news_data(df_news)
                # è¿™é‡Œ df_news åªæœ‰å½“å¤©æ•°æ®ï¼Œkey_col='date' ä¼šç”Ÿæˆå¦‚ 2025-12-27.parquet
                storage.save_partitioned(df_news, "alt_cctv_news", key_col='date')
        except: pass
            
        # 4.2 è¡Œä¸šå¸‚ç›ˆç‡ (æŒ‰æ—¥å­˜å‚¨)
        try:
            df_pe = ak_fetcher.fetch_industry_pe_snapshot(date_str)
            if not df_pe.empty:
                if 'å˜åŠ¨æ—¥æœŸ' in df_pe.columns:
                    df_pe.rename(columns={'å˜åŠ¨æ—¥æœŸ': 'date'}, inplace=True)
                    df_pe = cleaner.normalize_date(df_pe)
                    # ç”Ÿæˆå¦‚ 2025-12-27.parquet
                    storage.save_partitioned(df_pe, "industry_pe_daily", key_col='date')
        except: pass

# ==========================================
# ä¸»å…¥å£
# ==========================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AlphaFactorLab Data Updater")
    parser.add_argument('--mode', type=str, choices=['full', 'update'], default='update', help='full: å…¨é‡å†å², update: é‡è·‘å½“å¹´æ•°æ®')
    parser.add_argument('--task', type=str, choices=['all', 'stock', 'etf', 'finance', 'alt'], default='all', help='æŒ‡å®šè¿è¡Œçš„ä»»åŠ¡')
    
    args = parser.parse_args()
    
    start_time = time.time()
    
    if args.task in ['all', 'stock']:
        run_stock_update(args.mode)
        
    if args.task in ['all', 'etf']:
        run_etf_update(args.mode)
        
    if args.task in ['all', 'finance']:
        run_finance_update(args.mode)
        
    if args.task in ['all', 'alt']:
        run_alt_update(args.mode)
        
    elapsed = time.time() - start_time
    logger.info(f"ğŸ‰ All tasks completed in {elapsed:.2f} seconds.")