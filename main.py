# -*- coding: utf-8 -*-
"""
AlphaFactorLab æ•°æ®æ›´æ–°ä¸»ç¨‹åº (ç»†ç²’åº¦æ§åˆ¶ç‰ˆ)
åŠŸèƒ½: è°ƒåº¦å„ä¸ª Fetcherï¼Œæ¸…æ´—æ•°æ®ï¼Œå¹¶å­˜å‚¨ä¸º Hive Partition Parquet
ç”¨æ³•ç¤ºä¾‹:
    python main.py --mode update --task alt_industry_pe  <-- ä»…æ›´æ–°è¡Œä¸šå¸‚ç›ˆç‡
    python main.py --mode update --task alt_news         <-- ä»…æ›´æ–°æ–°é—»
    python main.py --mode update --task stock            <-- æ›´æ–°è‚¡ç¥¨
"""

import argparse
import datetime
import time
from tqdm import tqdm
from typing import Tuple
import warnings
warnings.filterwarnings("ignore")

# --- å¯¼å…¥é…ç½® ---
from config.settings import ETF_POOL, INDEX_POOL, START_DATE_FULL, PROCESSED_DIR

# å¼•å…¥æ ¸å¿ƒæ¨¡å—
from src.fetchers.baostock_api import BaostockFetcher
from src.fetchers.akshare_api import AkshareFetcher
from src.fetchers.mootdx_api import MootdxFetcher
from src.processors.cleaner import DataCleaner
from src.storage.parquet_manager import ParquetStorage
from src.utils.logger import get_logger

# é…ç½®æ—¥å¿—
logger = get_logger("Main", "data_update.log")

def get_date_range(mode: str) -> Tuple[str, str]:
    """è®¡ç®—æ—¶é—´èŒƒå›´: updateæ¨¡å¼å›æº¯åˆ°å½“å¹´1æœˆ1æ—¥"""
    end_date = datetime.date.today().strftime('%Y-%m-%d')
    if mode == 'full':
        start_date = START_DATE_FULL
    else:
        current_year = datetime.date.today().year
        start_date = f"{current_year}-01-01"
    return start_date, end_date

# ==========================================
# 1. ğŸ“ˆ è‚¡ç¥¨ä¸æŒ‡æ•° (Baostock)
# ==========================================
def run_stock_update(mode: str):
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting STOCK update ({mode}): {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    
    with BaostockFetcher() as bs:
        # 1.1 æŒ‡æ•°
        logger.info("Updating Indexes...")
        for code in INDEX_POOL:
            df = bs.fetch_index_kline(code, start_date, end_date)
            if not df.empty:
                df = cleaner.clean_daily_market_data(df)
                storage.save_partitioned(df, "index_price_daily", key_col='code')
        
        # 1.2 ä¸ªè‚¡
        raw_codes = bs.fetch_all_stock_codes()
        stock_codes = [c for c in raw_codes if not (c.startswith("sh.000") or c.startswith("sz.399"))]
        logger.info(f"Found {len(stock_codes)} stocks.")
        
        for code in tqdm(stock_codes, desc="Stocks"):
            try:
                df = bs.fetch_daily_kline(code, start_date, end_date, adjust='1')
                if not df.empty:
                    df = cleaner.clean_daily_market_data(df)
                    storage.save_partitioned(df, "stock_price_daily", key_col='code')
            except Exception as e:
                logger.error(f"Failed stock {code}: {e}")

# ==========================================
# 2. ğŸ“Š ETF (Mootdx)
# ==========================================
def run_etf_update(mode: str):
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting ETF update ({mode}): {start_date} -> {end_date}")
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    
    with MootdxFetcher() as mdx:
        for name, (code, ipo_year) in tqdm(ETF_POOL.items(), desc="ETFs"):
            try:
                df = mdx.fetch_etf_daily_kline(code, ipo_year, start_date, end_date, adjust_factor='02')
                if not df.empty:
                    df['name'] = name 
                    df = cleaner.clean_daily_market_data(df)
                    storage.save_partitioned(df, "etf_price_daily", key_col='name')
            except Exception as e:
                logger.error(f"Failed ETF {name}: {e}")

# ==========================================
# 3. ğŸ’° è´¢åŠ¡ä¸æ¦‚å¿µ (Akshare)
# ==========================================
def run_finance_update(mode: str):
    logger.info(f"ğŸš€ Starting FINANCE & CONCEPT update")
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    # 3.1 è´¢åŠ¡
    with BaostockFetcher() as bs:
        raw_codes = bs.fetch_all_stock_codes()
        stock_codes = [c for c in raw_codes if not (c.startswith("sh.000") or c.startswith("sz.399"))]
        
    logger.info(f"Updating Financial Reports for {len(stock_codes)} stocks...")
    for code in tqdm(stock_codes, desc="Finance"):
        try:
            df = ak_fetcher.fetch_financial_report(code)
            if not df.empty:
                df = cleaner.clean_financial_report(df)
                storage.save_partitioned(df, "stock_financial", partition_col="report_date", key_col='code')
        except: pass

    # 3.2 æ¦‚å¿µ
    df_concepts = ak_fetcher.fetch_concept_boards()
    if not df_concepts.empty:
        start_date, end_date = get_date_range(mode)
        start_str = start_date.replace("-", "")
        end_str = end_date.replace("-", "")
        
        for _, row in tqdm(df_concepts.iterrows(), total=len(df_concepts), desc="Concept Daily"):
            name = row['name']
            try:
                df_daily = ak_fetcher.fetch_concept_daily(name, start_str, end_str)
                if not df_daily.empty:
                    df_daily['concept_name'] = name
                    df_daily = cleaner.clean_daily_market_data(df_daily)
                    storage.save_partitioned(df_daily, "concept_price_daily", key_col='concept_name')
                time.sleep(0.5) 
            except: pass

# ==========================================
# 4. ğŸ—ï¸ å¦ç±»æ•°æ® (æ‹†åˆ†ä¸ºç‹¬ç«‹ä»»åŠ¡)
# ==========================================

def run_alt_news(mode: str):
    """ä»»åŠ¡: ä»…ä»…æ›´æ–°æ–°é—»è”æ’­"""
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting ALT: CCTV News update: {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    start = datetime.datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.datetime.strptime(end_date, "%Y-%m-%d")
    date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days + 1)]
    
    for date_obj in tqdm(date_generated, desc="CCTV News"):
        date_str = date_obj.strftime("%Y%m%d")
        try:
            df_news = ak_fetcher.fetch_cctv_news(date_str)
            if not df_news.empty:
                df_news = cleaner.clean_news_data(df_news)
                storage.save_partitioned(df_news, "alt_cctv_news", key_col='date')
        except: pass

def run_alt_industry_pe(mode: str):
    """ä»»åŠ¡: ä»…æ›´æ–°è¡Œä¸šå¸‚ç›ˆç‡"""
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting ALT: Industry PE update: {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    start = datetime.datetime.strptime('2023-05-19', "%Y-%m-%d") # è¡Œä¸šPEæ•°æ®èµ·å§‹æ—¥æœŸï¼Œå¯èƒ½ä¼šé€æ¸æ¨å
    end = datetime.datetime.strptime(end_date, "%Y-%m-%d")
    date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days + 1)]
    
    for date_obj in tqdm(date_generated, desc="Industry PE"):
        date_str = date_obj.strftime("%Y%m%d")
        try:
            df_pe = ak_fetcher.fetch_industry_pe_snapshot(date_str)
            if not df_pe.empty:
                if 'å˜åŠ¨æ—¥æœŸ' in df_pe.columns:
                    df_pe.rename(columns={'å˜åŠ¨æ—¥æœŸ': 'date'}, inplace=True)
                    df_pe = cleaner.normalize_date(df_pe)
                    storage.save_partitioned(df_pe, "industry_pe_daily", key_col='date')
        except: pass

def run_alt_market_metric(mode: str):
    """ä»»åŠ¡: æ›´æ–°å…¨å¸‚åœºä¼°å€¼ (PE/PB)"""
    logger.info(f"ğŸš€ Starting ALT: Market Metrics (PE/PB) update")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    try:
        # 1. å¸‚åœºPE
        logger.info("Fetching Market PE...")
        df_pe = ak_fetcher.fetch_market_pe()
        if not df_pe.empty and 'date' in df_pe.columns:
            df_pe = cleaner.normalize_date(df_pe)
            storage.save_partitioned(df_pe, "market_pe_lg", key_col='date')

        # 2. å¸‚åœºPB
        logger.info("Fetching Market PB...")
        df_pb = ak_fetcher.fetch_market_pb()
        if not df_pb.empty and 'date' in df_pb.columns:
            df_pb = cleaner.normalize_date(df_pb)
            storage.save_partitioned(df_pb, "market_pb_all", key_col='date')
            
    except Exception as e:
        logger.error(f"Failed to update market metrics: {e}")

def run_alt_all(mode: str):
    """ä»»åŠ¡: æ›´æ–°æ‰€æœ‰å¦ç±»æ•°æ®"""
    run_alt_news(mode)
    run_alt_industry_pe(mode)
    run_alt_market_metric(mode)

# ==========================================
# ä¸»å…¥å£
# ==========================================
if __name__ == "__main__":
    # å®šä¹‰æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨
    TASKS = [
        'all',           # è·‘æ‰€æœ‰
        'stock',         # ä»…è‚¡ç¥¨
        'etf',           # ä»…ETF
        'finance',       # ä»…è´¢åŠ¡+æ¦‚å¿µ
        'alt',           # æ‰€æœ‰å¦ç±»æ•°æ®
        'alt_news',      # [æ–°å¢] ä»…æ–°é—»
        'alt_industry_pe', # [æ–°å¢] ä»…è¡Œä¸šPE
        'alt_market_metric' # [æ–°å¢] ä»…å¸‚åœºæ•´ä½“ä¼°å€¼
    ]

    parser = argparse.ArgumentParser(description="AlphaFactorLab Data Updater")
    parser.add_argument('--mode', type=str, choices=['full', 'update'], default='update', help='full: å…¨é‡, update: å½“å¹´å¢é‡')
    parser.add_argument('--task', type=str, choices=TASKS, default='all', help='æŒ‡å®šè¿è¡Œçš„ä»»åŠ¡')
    
    args = parser.parse_args()
    
    start_time = time.time()
    
    # ä»»åŠ¡è°ƒåº¦é€»è¾‘
    if args.task == 'all':
        run_stock_update(args.mode)
        run_etf_update(args.mode)
        run_finance_update(args.mode)
        run_alt_all(args.mode)
        
    elif args.task == 'stock':
        run_stock_update(args.mode)
        
    elif args.task == 'etf':
        run_etf_update(args.mode)
        
    elif args.task == 'finance':
        run_finance_update(args.mode)
        
    elif args.task == 'alt':
        run_alt_all(args.mode)
        
    elif args.task == 'alt_news':
        run_alt_news(args.mode)
        
    elif args.task == 'alt_industry_pe':
        run_alt_industry_pe(args.mode)
        
    elif args.task == 'alt_market_metric':
        run_alt_market_metric(args.mode)
        
    elapsed = time.time() - start_time
    logger.info(f"ğŸ‰ Task '{args.task}' completed in {elapsed:.2f} seconds.")