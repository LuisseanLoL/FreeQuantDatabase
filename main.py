# -*- coding: utf-8 -*-
"""
AlphaFactorLab æ•°æ®æ›´æ–°ä¸»ç¨‹åº (é«˜åº¦è§£è€¦ç‰ˆ)
åŠŸèƒ½: è°ƒåº¦å„ä¸ª Fetcherï¼Œæ¸…æ´—æ•°æ®ï¼Œå¹¶å­˜å‚¨ä¸º Hive Partition Parquet
ç”¨æ³•ç¤ºä¾‹:
    python main.py --mode update --task concept     <-- ä»…æ›´æ–°æ¦‚å¿µæ¿å—
    python main.py --mode update --task index       <-- ä»…æ›´æ–°å¤§ç›˜æŒ‡æ•°
    python main.py --mode update --task stock       <-- æ›´æ–°ä¸ªè‚¡è¡Œæƒ…
"""

import argparse
import datetime
import time
from tqdm import tqdm
from typing import Tuple
import warnings
warnings.filterwarnings("ignore")

# --- å¯¼å…¥é…ç½® ---
from config.settings import ETF_POOL, INDEX_POOL, START_DATE_FULL, PROCESSED_DIR

# å¼•å…¥æ ¸å¿ƒæ¨¡å—
from src.fetchers.baostock_api import BaostockFetcher
from src.fetchers.akshare_api import AkshareFetcher
from src.fetchers.mootdx_api import MootdxFetcher
from src.processors.cleaner import DataCleaner
from src.storage.parquet_manager import ParquetStorage
from src.utils.logger import get_logger

# é…ç½®æ—¥å¿—
logger = get_logger("Main", "data_update.log")

def get_date_range(mode: str) -> Tuple[str, str]:
    """è®¡ç®—æ—¶é—´èŒƒå›´: updateæ¨¡å¼å›æº¯åˆ°å½“å¹´1æœˆ1æ—¥"""
    end_date = datetime.date.today().strftime('%Y-%m-%d')
    if mode == 'full':
        start_date = START_DATE_FULL
    else:
        current_year = datetime.date.today().year
        start_date = f"{current_year}-01-01"
    return start_date, end_date

# ==========================================
# 1. ğŸ“ˆ æŒ‡æ•° (Index) - [ç‹¬ç«‹æ‹†åˆ†]
# ==========================================
def run_index_update(mode: str):
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting INDEX update ({mode}): {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    
    with BaostockFetcher() as bs:
        logger.info(f"Updating {len(INDEX_POOL)} Indexes...")
        for code in INDEX_POOL:
            # è·å–æŒ‡æ•°æ—¥çº¿ (ä¸å¤æƒ)
            df = bs.fetch_index_kline(code, start_date, end_date)
            if not df.empty:
                df = cleaner.clean_daily_market_data(df)
                storage.save_partitioned(df, "index_price_daily", key_col='code')

# ==========================================
# 2. ğŸ“ˆ ä¸ªè‚¡ (Stock)
# ==========================================
def run_stock_update(mode: str):
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting STOCK update ({mode}): {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    
    with BaostockFetcher() as bs:
        raw_codes = bs.fetch_all_stock_codes()
        # ä¸¥æ ¼è¿‡æ»¤ï¼Œåªä¿ç•™ä¸ªè‚¡
        stock_codes = [c for c in raw_codes if not (c.startswith("sh.000") or c.startswith("sz.399"))]
        logger.info(f"Found {len(stock_codes)} stocks.")
        
        for code in tqdm(stock_codes, desc="Stocks"):
            try:
                # adjust='1' åå¤æƒ
                df = bs.fetch_daily_kline(code, start_date, end_date, adjust='1')
                if not df.empty:
                    df = cleaner.clean_daily_market_data(df)
                    storage.save_partitioned(df, "stock_price_daily", key_col='code')
            except Exception as e:
                logger.error(f"Failed stock {code}: {e}")

# ==========================================
# 3. ğŸ“Š ETF (Mootdx)
# ==========================================
def run_etf_update(mode: str):
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting ETF update ({mode}): {start_date} -> {end_date}")
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    
    with MootdxFetcher() as mdx:
        for name, (code, ipo_year) in tqdm(ETF_POOL.items(), desc="ETFs"):
            try:
                df = mdx.fetch_etf_daily_kline(code, ipo_year, start_date, end_date, adjust_factor='02')
                if not df.empty:
                    df['name'] = name 
                    df = cleaner.clean_daily_market_data(df)
                    storage.save_partitioned(df, "etf_price_daily", key_col='name')
            except Exception as e:
                logger.error(f"Failed ETF {name}: {e}")

# ==========================================
# 4. ğŸ’° è´¢åŠ¡æŠ¥è¡¨ (Finance)
# ==========================================
def run_finance_update(mode: str):
    """ä»…æ›´æ–°è´¢åŠ¡æŠ¥è¡¨ï¼Œä¸å†åŒ…å«æ¦‚å¿µæ•°æ®"""
    logger.info(f"ğŸš€ Starting FINANCIAL REPORT update")
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    # å€ŸåŠ© Baostock è·å–æœ€æ–°çš„è‚¡ç¥¨åˆ—è¡¨
    with BaostockFetcher() as bs:
        raw_codes = bs.fetch_all_stock_codes()
        stock_codes = [c for c in raw_codes if not (c.startswith("sh.000") or c.startswith("sz.399"))]
        
    logger.info(f"Updating Financial Reports for {len(stock_codes)} stocks...")
    for code in tqdm(stock_codes, desc="Finance"):
        try:
            df = ak_fetcher.fetch_financial_report(code)
            if not df.empty:
                df = cleaner.clean_financial_report(df)
                storage.save_partitioned(df, "stock_financial", partition_col="report_date", key_col='code')
        except: pass

# ==========================================
# 5. ğŸ’¡ æ¦‚å¿µæ¿å— (Concept) - [ç‹¬ç«‹æ‹†åˆ†]
# ==========================================
def run_concept_update(mode: str):
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting CONCEPT update ({mode}): {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    df_concepts = ak_fetcher.fetch_concept_boards()
    if df_concepts.empty:
        logger.warning("No concept boards found.")
        return

    # Akshare æ¦‚å¿µæ¥å£é€šå¸¸éœ€è¦ YYYYMMDD
    start_str = start_date.replace("-", "")
    end_str = end_date.replace("-", "")
    
    logger.info(f"Updating {len(df_concepts)} Concept Boards...")
    for _, row in tqdm(df_concepts.iterrows(), total=len(df_concepts), desc="Concept"):
        name = row['name']
        try:
            df_daily = ak_fetcher.fetch_concept_daily(name, start_str, end_str)
            if not df_daily.empty:
                df_daily['concept_name'] = name
                df_daily = cleaner.clean_daily_market_data(df_daily)
                storage.save_partitioned(df_daily, "concept_price_daily", key_col='concept_name')
            
            # é€‚å½“é™æµï¼ŒåŒèŠ±é¡ºæ¥å£è¾ƒä¸ºæ•æ„Ÿ
            time.sleep(0.3) 
        except: pass

# ==========================================
# 6. ğŸ—ï¸ å¦ç±»æ•°æ® (Alternative)
# ==========================================

def run_alt_news(mode: str):
    """ä»…æ›´æ–°æ–°é—»è”æ’­"""
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting ALT: CCTV News update: {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    start = datetime.datetime.strptime('2016-03-30', "%Y-%m-%d") 
    end = datetime.datetime.strptime(end_date, "%Y-%m-%d")
    date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days + 1)]
    
    for date_obj in tqdm(date_generated, desc="CCTV News"):
        date_str = date_obj.strftime("%Y%m%d")
        try:
            df_news = ak_fetcher.fetch_cctv_news(date_str)
            if not df_news.empty:
                df_news = cleaner.clean_news_data(df_news)
                storage.save_partitioned(df_news, "alt_cctv_news", key_col='date')
        except: pass

def run_alt_industry_pe(mode: str):
    """ä»…æ›´æ–°è¡Œä¸šå¸‚ç›ˆç‡"""
    start_date, end_date = get_date_range(mode)
    logger.info(f"ğŸš€ Starting ALT: Industry PE update: {start_date} -> {end_date}")
    
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    start = datetime.datetime.strptime('2023-05-19', "%Y-%m-%d")
    end = datetime.datetime.strptime(end_date, "%Y-%m-%d")
    date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days + 1)]
    
    for date_obj in tqdm(date_generated, desc="Industry PE"):
        date_str = date_obj.strftime("%Y%m%d")
        try:
            df_pe = ak_fetcher.fetch_industry_pe_snapshot(date_str)
            if not df_pe.empty:
                if 'å˜åŠ¨æ—¥æœŸ' in df_pe.columns:
                    df_pe.rename(columns={'å˜åŠ¨æ—¥æœŸ': 'date'}, inplace=True)
                    df_pe = cleaner.normalize_date(df_pe)
                    storage.save_partitioned(df_pe, "industry_pe_daily", key_col='date')
        except: pass

def run_alt_market_metric(mode: str):
    """æ›´æ–°å…¨å¸‚åœºä¼°å€¼ (PE/PB)"""
    logger.info(f"ğŸš€ Starting ALT: Market Metrics (PE/PB) update")
    storage = ParquetStorage(PROCESSED_DIR)
    cleaner = DataCleaner()
    ak_fetcher = AkshareFetcher()
    
    try:
        df_pe = ak_fetcher.fetch_market_pe()
        if not df_pe.empty and 'date' in df_pe.columns:
            df_pe = cleaner.normalize_date(df_pe)
            storage.save_partitioned(df_pe, "market_pe_lg", key_col='date')

        df_pb = ak_fetcher.fetch_market_pb()
        if not df_pb.empty and 'date' in df_pb.columns:
            df_pb = cleaner.normalize_date(df_pb)
            storage.save_partitioned(df_pb, "market_pb_all", key_col='date')
    except Exception as e:
        logger.error(f"Failed to update market metrics: {e}")

def run_alt_all(mode: str):
    run_alt_news(mode)
    run_alt_industry_pe(mode)
    run_alt_market_metric(mode)

# ==========================================
# ä¸»å…¥å£
# ==========================================
if __name__ == "__main__":
    # å®šä¹‰æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨
    TASKS = [
        'all',             # è·‘æ‰€æœ‰
        'stock',           # ä¸ªè‚¡è¡Œæƒ…
        'index',           # [æ–°å¢] æŒ‡æ•°è¡Œæƒ…
        'etf',             # ETFè¡Œæƒ…
        'finance',         # è´¢åŠ¡æŠ¥è¡¨
        'concept',         # [æ–°å¢] æ¦‚å¿µæ¿å—
        'alt',             # æ‰€æœ‰å¦ç±»æ•°æ®
        'alt_news',        # ä»…æ–°é—»
        'alt_industry_pe', # ä»…è¡Œä¸šPE
        'alt_market_metric'# ä»…å¸‚åœºä¼°å€¼
    ]

    parser = argparse.ArgumentParser(description="AlphaFactorLab Data Updater")
    parser.add_argument('--mode', type=str, choices=['full', 'update'], default='update', help='full: å…¨é‡, update: å½“å¹´å¢é‡')
    parser.add_argument('--task', type=str, choices=TASKS, default='all', help='æŒ‡å®šè¿è¡Œçš„ä»»åŠ¡')
    
    args = parser.parse_args()
    
    start_time = time.time()
    
    # ä»»åŠ¡è°ƒåº¦é€»è¾‘
    if args.task == 'all':
        run_index_update(args.mode) # å…ˆè·‘æŒ‡æ•°ï¼Œé€Ÿåº¦å¿«
        run_stock_update(args.mode)
        run_etf_update(args.mode)
        run_finance_update(args.mode)
        run_concept_update(args.mode)
        run_alt_all(args.mode)
    
    elif args.task == 'index':
        run_index_update(args.mode)

    elif args.task == 'stock':
        run_stock_update(args.mode)
        
    elif args.task == 'etf':
        run_etf_update(args.mode)
        
    elif args.task == 'finance':
        run_finance_update(args.mode)
        
    elif args.task == 'concept':
        run_concept_update(args.mode)
        
    elif args.task == 'alt':
        run_alt_all(args.mode)
        
    elif args.task == 'alt_news':
        run_alt_news(args.mode)
        
    elif args.task == 'alt_industry_pe':
        run_alt_industry_pe(args.mode)
        
    elif args.task == 'alt_market_metric':
        run_alt_market_metric(args.mode)
        
    elapsed = time.time() - start_time
    logger.info(f"ğŸ‰ Task '{args.task}' completed in {elapsed:.2f} seconds.")