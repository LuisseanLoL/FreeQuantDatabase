# -*- coding: utf-8 -*-
"""
Parquet å­˜å‚¨ç®¡ç†å™¨ (ç²¾ç¡®è¦†ç›–ç‰ˆ)
è·¯å¾„: src/storage/parquet_manager.py
åŠŸèƒ½: 
    1. æ‰‹åŠ¨ç®¡ç† Hive åˆ†åŒºè·¯å¾„
    2. å¼ºåˆ¶ä½¿ç”¨ "{code}.parquet" å‘½åæ–‡ä»¶ï¼Œå®ç°ç²¾ç¡®è¦†ç›–
"""

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import sys
import os
from pathlib import Path

# ğŸš‘ è·¯å¾„è¡¥ä¸
project_root = str(Path(__file__).resolve().parents[2])
if project_root not in sys.path:
    sys.path.append(project_root)

from src.utils.logger import get_logger

logger = get_logger(__name__, "storage.log")

class ParquetStorage:
    def __init__(self, base_dir: str = "data/processed"):
        if not os.path.isabs(base_dir):
            self.base_dir = Path(project_root) / base_dir
        else:
            self.base_dir = Path(base_dir)
            
        if not self.base_dir.exists():
            self.base_dir.mkdir(parents=True, exist_ok=True)

    def save_partitioned(self, 
                         df: pd.DataFrame, 
                         category: str, 
                         partition_col: str = 'date',
                         key_col: str = 'code'):
        """
        ä¿å­˜æ•°æ®ï¼Œå¼ºåˆ¶æ–‡ä»¶åä¸º "{key_col}.parquet" (ä¾‹å¦‚ sh.600000.parquet)
        
        :param df: æ•°æ® DataFrame
        :param category: æ•°æ®ç±»åˆ« (å¦‚ stock_price_daily)
        :param partition_col: æ—¶é—´åˆ—ï¼Œç”¨äºæå–å¹´ä»½ (å¦‚ date)
        :param key_col: ç”¨äºå‘½åçš„å…³é”®åˆ— (å¦‚ code, name)
        """
        if df.empty: return

        if partition_col not in df.columns:
            logger.error(f"âŒ Partition col '{partition_col}' missing")
            return
        
        if key_col not in df.columns:
            logger.error(f"âŒ Key col '{key_col}' missing (needed for filename)")
            return

        # 1. é¢„å¤„ç†ï¼šæå–å¹´ä»½
        temp_date = pd.to_datetime(df[partition_col], errors='coerce')
        df = df.copy()
        df['year'] = temp_date.dt.year
        df = df.dropna(subset=['year'])
        df['year'] = df['year'].astype(int)

        # 2. æŒ‰å¹´ä»½åˆ†ç»„å¤„ç† (é€šå¸¸ä¼ å…¥çš„dfæ˜¯å•åªè‚¡ç¥¨å¤šå¹´çš„æ•°æ®)
        # è¿™æ ·å¯ä»¥æ­£ç¡®åœ°æŠŠ 2024å¹´çš„æ•°æ®å­˜å…¥ year=2024, 2025å¹´çš„å­˜å…¥ year=2025
        for year, group in df.groupby('year'):
            # ç›®æ ‡æ–‡ä»¶å¤¹: data/processed/stock_price_daily/year=2025
            year_dir = self.base_dir / category / f"year={year}"
            if not year_dir.exists():
                year_dir.mkdir(parents=True, exist_ok=True)

            # 3. æ„é€ ç¡®å®šçš„æ–‡ä»¶å
            # è·å–è¯¥ç»„æ•°æ®çš„å”¯ä¸€æ ‡è¯† (ä¾‹å¦‚ sh.600000)
            # æˆ‘ä»¬å‡è®¾ä¼ å…¥çš„ df æ˜¯ä¸€åªè‚¡ç¥¨çš„æ•°æ®ï¼Œæ‰€ä»¥ç›´æ¥å–ç¬¬ä¸€è¡Œçš„ code
            unique_key = str(group[key_col].iloc[0])
            
            # æ–‡ä»¶å: sh.600000.parquet
            # æ³¨æ„å¤„ç†æ–‡ä»¶åä¸­å¯èƒ½å­˜åœ¨çš„éæ³•å­—ç¬¦ (å¦‚ : / \)
            safe_filename = unique_key.replace("/", "_").replace("\\", "_") + ".parquet"
            file_path = year_dir / safe_filename

            try:
                # 4. å†™å…¥ (PyArrow ä¼šç›´æ¥è¦†ç›–åŒåæ–‡ä»¶)
                table = pa.Table.from_pandas(group)
                pq.write_table(table, file_path, compression='snappy')
                
                # logger.info(f"ğŸ’¾ Saved {safe_filename} to year={year}") # æ—¥å¿—å¤ªåˆ·å±å¯æ³¨é‡Š
                
            except Exception as e:
                logger.error(f"âŒ Failed to write {file_path}: {e}")